{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6UxEkLc6yz6J"
   },
   "source": [
    "# SI 650 / EECS 549: Homework 3 Part 1\n",
    "## Introduction to PyTerrier \n",
    "\n",
    "This homework is intended to expose you to other types of information retrieval and demonstrates the use of another state of the art IR library, [PyTerrier](https://github.com/terrier-org/pyterrier). \n",
    "\n",
    "The overall learning goals of the assignment across all three parts are\n",
    "  - Learn how to use PyTerrier\n",
    "  - Understand how to train and use a Learning to Rank model\n",
    "  - Understand how to train and use a dense vector retrieval (using deep learning)\n",
    "  - Understand how to use document augmentation\n",
    "  - Gain additional programming and debugging skills when working with modern IR libraries\n",
    "  - Learn how to use the [Great Lakes cluster](https://arc.umich.edu/greatlakes/)\n",
    "  \n",
    "  \n",
    "The Great Lakes cluster is a collection of high performance computers at the University of Michigan. The big advantage for this course is the ability to use its GPUs for doing deep learning. You will have access to this cluster for Homework 3 _and_ for your course project, which can expand the type of methods you can try. When launching jobs for this course, be sure to have your job use the `si650f21_class` account.\n",
    "\n",
    "For this assignment, we'll be using the [CORD19 test collection](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge), which is a collection of documents about Covid-19 produced by AI2. In places, we've pretrained models and precomputed indices for you (which can take large amounts of time), but we'll ask you to try out the commands on a small scale so you'll know how to run them.\n",
    "\n",
    "Homework 3 Part 1 will have you working on the following tasks to get you started:\n",
    "  - PyTerrier installation & configuration\n",
    "  - indexing a collection\n",
    "  - accessing an index\n",
    "  - using the `BatchRetrieve` transformer for searching an index\n",
    "  - conducting an `Experiment` \n",
    "\n",
    "For all parts of the homework, you can run them on your local computer with enough time. However, for Part 3, you will see *significant* speed up running these as notebooks on Great Lakes with a GPU. The three parts are designed to be completed in order, as they build on each other conceptually.\n",
    "\n",
    "For each notebook, all the tasks that you will need to complete are marked with **Task** in a cell title comment.\n",
    "\n",
    "Note that just like Pyserini, PyTerrier also uses a Java-based  library underneath, [Terrier information retrieval toolkit](http://terrier.org), so you will need to set `JAVA_HOME` accordingly. underlying for many indexing and retrieval operations. PyTerrier is relatively new in 2020, but Terrier has a long history dating back to 2001 and  makes it easy to perform IR experiments in Python, which could come in handy for you when doing your course project.\n",
    "\n",
    "See the [PyTerrier documentation](https://pyterrier.readthedocs.io/en/latest/) for many more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7u2hD-zBzfpR"
   },
   "source": [
    "PyTerrier is a Python framework, but uses the underlying [Terrier information retrieval toolkit](http://terrier.org) for many indexing and retrieval operations. While PyTerrier was new in 2020, Terrier is written in Java and has a long history dating back to 2001. PyTerrier makes it easy to perform IR experiments in Python, but using the mature Terrier platform for the expensive indexing and retrieval operations. \n",
    "\n",
    "In the following, we introduce everything you need to know about PyTerrier, and also provide appropriate links to relevant parts of the [PyTerrier documentation](https://pyterrier.readthedocs.io/en/latest/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-terrier in /home/andy/anaconda3/lib/python3.8/site-packages (0.7.1)\n",
      "Requirement already satisfied: more-itertools in /home/andy/anaconda3/lib/python3.8/site-packages (from python-terrier) (8.6.0)\n",
      "Requirement already satisfied: joblib in /home/andy/.local/lib/python3.8/site-packages (from python-terrier) (1.1.0)\n",
      "Collecting pyjnius~=1.3.0\n",
      "  Using cached pyjnius-1.3.0-cp38-cp38-manylinux2010_x86_64.whl (1.4 MB)\n",
      "Requirement already satisfied: nptyping in /home/andy/anaconda3/lib/python3.8/site-packages (from python-terrier) (1.4.4)\n",
      "Requirement already satisfied: scipy in /home/andy/.local/lib/python3.8/site-packages (from python-terrier) (1.7.1)\n",
      "Requirement already satisfied: matchpy in /home/andy/anaconda3/lib/python3.8/site-packages (from python-terrier) (0.5.5)\n",
      "Requirement already satisfied: wget in /home/andy/anaconda3/lib/python3.8/site-packages (from python-terrier) (3.2)\n",
      "Requirement already satisfied: jinja2 in /home/andy/anaconda3/lib/python3.8/site-packages (from python-terrier) (2.11.2)\n",
      "Requirement already satisfied: chest in /home/andy/anaconda3/lib/python3.8/site-packages (from python-terrier) (0.2.3)\n",
      "Requirement already satisfied: requests in /home/andy/anaconda3/lib/python3.8/site-packages (from python-terrier) (2.24.0)\n",
      "Requirement already satisfied: sklearn in /home/andy/anaconda3/lib/python3.8/site-packages (from python-terrier) (0.0)\n",
      "Requirement already satisfied: dill in /home/andy/anaconda3/lib/python3.8/site-packages (from python-terrier) (0.3.4)\n",
      "Requirement already satisfied: pandas in /home/andy/.local/lib/python3.8/site-packages (from python-terrier) (1.3.3)\n",
      "Requirement already satisfied: ir-datasets>=0.3.2 in /home/andy/anaconda3/lib/python3.8/site-packages (from python-terrier) (0.4.3)\n",
      "Requirement already satisfied: tqdm in /home/andy/.local/lib/python3.8/site-packages (from python-terrier) (4.62.3)\n",
      "Requirement already satisfied: deprecation in /home/andy/anaconda3/lib/python3.8/site-packages (from python-terrier) (2.1.0)\n",
      "Requirement already satisfied: statsmodels in /home/andy/anaconda3/lib/python3.8/site-packages (from python-terrier) (0.12.0)\n",
      "Requirement already satisfied: numpy in /home/andy/.local/lib/python3.8/site-packages (from python-terrier) (1.21.2)\n",
      "Requirement already satisfied: ir-measures>=0.2.0 in /home/andy/anaconda3/lib/python3.8/site-packages (from python-terrier) (0.2.1)\n",
      "Requirement already satisfied: six>=1.7.0 in /home/andy/anaconda3/lib/python3.8/site-packages (from pyjnius~=1.3.0->python-terrier) (1.15.0)\n",
      "Requirement already satisfied: cython in /home/andy/.local/lib/python3.8/site-packages (from pyjnius~=1.3.0->python-terrier) (0.29.24)\n",
      "Requirement already satisfied: typish>=1.7.0 in /home/andy/anaconda3/lib/python3.8/site-packages (from nptyping->python-terrier) (1.9.3)\n",
      "Requirement already satisfied: multiset<3.0,>=2.0 in /home/andy/anaconda3/lib/python3.8/site-packages (from matchpy->python-terrier) (2.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/andy/anaconda3/lib/python3.8/site-packages (from jinja2->python-terrier) (1.1.1)\n",
      "Requirement already satisfied: heapdict in /home/andy/anaconda3/lib/python3.8/site-packages (from chest->python-terrier) (1.0.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/andy/anaconda3/lib/python3.8/site-packages (from requests->python-terrier) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/andy/anaconda3/lib/python3.8/site-packages (from requests->python-terrier) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/andy/anaconda3/lib/python3.8/site-packages (from requests->python-terrier) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/andy/anaconda3/lib/python3.8/site-packages (from requests->python-terrier) (2.10)\n",
      "Requirement already satisfied: scikit-learn in /home/andy/.local/lib/python3.8/site-packages (from sklearn->python-terrier) (1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/andy/.local/lib/python3.8/site-packages (from pandas->python-terrier) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/andy/.local/lib/python3.8/site-packages (from pandas->python-terrier) (2021.1)\n",
      "Requirement already satisfied: lxml>=4.5.2 in /home/andy/anaconda3/lib/python3.8/site-packages (from ir-datasets>=0.3.2->python-terrier) (4.6.1)\n",
      "Requirement already satisfied: lz4>=3.1.1 in /home/andy/anaconda3/lib/python3.8/site-packages (from ir-datasets>=0.3.2->python-terrier) (3.1.3)\n",
      "Requirement already satisfied: ijson>=3.1.3 in /home/andy/anaconda3/lib/python3.8/site-packages (from ir-datasets>=0.3.2->python-terrier) (3.1.4)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in /home/andy/anaconda3/lib/python3.8/site-packages (from ir-datasets>=0.3.2->python-terrier) (4.9.3)\n",
      "Requirement already satisfied: pyautocorpus>=0.1.1 in /home/andy/anaconda3/lib/python3.8/site-packages (from ir-datasets>=0.3.2->python-terrier) (0.1.6)\n",
      "Requirement already satisfied: warc3-wet-clueweb09>=0.2.5 in /home/andy/anaconda3/lib/python3.8/site-packages (from ir-datasets>=0.3.2->python-terrier) (0.2.5)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /home/andy/anaconda3/lib/python3.8/site-packages (from ir-datasets>=0.3.2->python-terrier) (5.3.1)\n",
      "Requirement already satisfied: zlib-state>=0.1.3 in /home/andy/anaconda3/lib/python3.8/site-packages (from ir-datasets>=0.3.2->python-terrier) (0.1.3)\n",
      "Requirement already satisfied: trec-car-tools>=2.5.4 in /home/andy/anaconda3/lib/python3.8/site-packages (from ir-datasets>=0.3.2->python-terrier) (2.5.4)\n",
      "Requirement already satisfied: warc3-wet>=0.2.3 in /home/andy/anaconda3/lib/python3.8/site-packages (from ir-datasets>=0.3.2->python-terrier) (0.2.3)\n",
      "Requirement already satisfied: packaging in /home/andy/.local/lib/python3.8/site-packages (from deprecation->python-terrier) (21.0)\n",
      "Requirement already satisfied: patsy>=0.5 in /home/andy/anaconda3/lib/python3.8/site-packages (from statsmodels->python-terrier) (0.5.1)\n",
      "Requirement already satisfied: pytrec-eval-terrier==0.5.1 in /home/andy/anaconda3/lib/python3.8/site-packages (from ir-measures>=0.2.0->python-terrier) (0.5.1)\n",
      "Requirement already satisfied: cwl-eval>=1.0.10 in /home/andy/anaconda3/lib/python3.8/site-packages (from ir-measures>=0.2.0->python-terrier) (1.0.10)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/andy/.local/lib/python3.8/site-packages (from scikit-learn->sklearn->python-terrier) (3.0.0)\n",
      "Requirement already satisfied: soupsieve>1.2; python_version >= \"3.0\" in /home/andy/anaconda3/lib/python3.8/site-packages (from beautifulsoup4>=4.4.1->ir-datasets>=0.3.2->python-terrier) (2.0.1)\n",
      "Requirement already satisfied: cbor>=1.0.0 in /home/andy/anaconda3/lib/python3.8/site-packages (from trec-car-tools>=2.5.4->ir-datasets>=0.3.2->python-terrier) (1.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/andy/.local/lib/python3.8/site-packages (from packaging->deprecation->python-terrier) (2.4.7)\n",
      "Installing collected packages: pyjnius\n",
      "  Attempting uninstall: pyjnius\n",
      "    Found existing installation: pyjnius 1.4.0\n",
      "    Uninstalling pyjnius-1.4.0:\n"
     ]
    }
   ],
   "source": [
    "!pip install python-terrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyterrier'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-08361fa14824>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'display.max_colwidth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m150\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyterrier\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyterrier'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Helpful for showing indexing information\n",
    "pd.set_option('display.max_colwidth', 150)\n",
    "\n",
    "import pyterrier as pt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iH0Ds2370V0G"
   },
   "source": [
    "### Starting PyTerrier\n",
    "\n",
    "The first step is to initialize PyTerrier using PyTerrier's `init()` method. The `init()` method will download Terrier's jar file (if it's not already) and then start the Java Virtual Machine. To avoid downstream complications, we check `started()` prior to calling `init()` to prevent multiple Terrier instances from running concurrently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17656,
     "status": "ok",
     "timestamp": 1615971633689,
     "user": {
      "displayName": "Nicola Tonellotto",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gga8BaxLsPFWvzzBzSximki7T2Jsnf0EEARTd_h=s64",
      "userId": "17533833776178224794"
     },
     "user_tz": -60
    },
    "id": "Z4qALBa90-7g",
    "outputId": "378c8773-686c-4abc-d1b5-af80d18cfed0"
   },
   "outputs": [],
   "source": [
    "if not pt.started():\n",
    "    pt.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-qqjVSu-5_FX"
   },
   "source": [
    "### Documents, Indexing and Indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3soS1IIy5B83"
   },
   "source": [
    "PyTerrier typically works with Pandas dataframes for inputs. Let's create a toy set of documents in a dataframe to test. Note that the column name of `docno` is a special PyTerrier name that is the unique identifier for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "executionInfo": {
     "elapsed": 14933,
     "status": "ok",
     "timestamp": 1615971633692,
     "user": {
      "displayName": "Nicola Tonellotto",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gga8BaxLsPFWvzzBzSximki7T2Jsnf0EEARTd_h=s64",
      "userId": "17533833776178224794"
     },
     "user_tz": -60
    },
    "id": "gSEiEuTE5uyL",
    "outputId": "8ef282f5-94df-403f-c501-6147f62d2de8"
   },
   "outputs": [],
   "source": [
    "docs_df = pd.DataFrame([\n",
    "        [\"d1\", \"this is the first document of many documents\"],\n",
    "        [\"d2\", \"this is another document\"],\n",
    "        [\"d3\", \"the topic of this document is unknown\"]\n",
    "    ], columns=[\"docno\", \"text\"])\n",
    "\n",
    "docs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2RCtCCTU6GAj"
   },
   "source": [
    "Before any search engine can estimate which documents are most likely to be relevant for a given query, it must index the documents. \n",
    "\n",
    "In the following cell, we index the dataframe's documents. The index, with all its data structures, is written into a directory called `toydocs_index`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 37
    },
    "executionInfo": {
     "elapsed": 2312,
     "status": "ok",
     "timestamp": 1615971681350,
     "user": {
      "displayName": "Nicola Tonellotto",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gga8BaxLsPFWvzzBzSximki7T2Jsnf0EEARTd_h=s64",
      "userId": "17533833776178224794"
     },
     "user_tz": -60
    },
    "id": "1YvLhEOS6V8w",
    "outputId": "ebe4070d-7160-42ba-bbfe-e5ccc2f3165b"
   },
   "outputs": [],
   "source": [
    "index_dir = './toydocs_index'\n",
    "indexer = pt.DFIndexer(index_dir, overwrite=True)\n",
    "index_ref = indexer.index(docs_df[\"text\"], docs_df[\"docno\"])\n",
    "index_ref.toString()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TUm6r6_625gW"
   },
   "source": [
    "PyTerrier will generate a index in the `toydocs_index` directory and and we can list the files to see what kind of internal structure and files it made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 581,
     "status": "ok",
     "timestamp": 1615971697027,
     "user": {
      "displayName": "Nicola Tonellotto",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gga8BaxLsPFWvzzBzSximki7T2Jsnf0EEARTd_h=s64",
      "userId": "17533833776178224794"
     },
     "user_tz": -60
    },
    "id": "TF45pl5O8p7R",
    "outputId": "c039d811-aebe-4e4d-f1d4-18b2b989d4c5"
   },
   "outputs": [],
   "source": [
    "os.listdir(index_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B2b8isFP3Kv6"
   },
   "source": [
    "Once we've generated the files associated with `index_ref`, we can load the information into an actual PyTerrier index using the method `pt.IndexFactory.of()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 587,
     "status": "ok",
     "timestamp": 1615971763026,
     "user": {
      "displayName": "Nicola Tonellotto",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gga8BaxLsPFWvzzBzSximki7T2Jsnf0EEARTd_h=s64",
      "userId": "17533833776178224794"
     },
     "user_tz": -60
    },
    "id": "TTM17szD6pNy",
    "outputId": "0d00514e-b30f-4b0c-a991-d618d24bb756"
   },
   "outputs": [],
   "source": [
    "index = pt.IndexFactory.of(index_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mZe3HD5i7G3v"
   },
   "source": [
    "See Terrier's [`Index`](http://terrier.org/docs/current/javadoc/org/terrier/structures/Index.html) object for documentation, which is written in Java's Javadoc format. We can call these methods on our index object as well. Important methods to note are:\n",
    " - `getCollectionStatistics()`\n",
    " - `getInvertedIndex()`\n",
    " - `getLexicon()`\n",
    "\n",
    "Let's see what is returned by the `CollectionStatistics()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 629,
     "status": "ok",
     "timestamp": 1615972787602,
     "user": {
      "displayName": "Nicola Tonellotto",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gga8BaxLsPFWvzzBzSximki7T2Jsnf0EEARTd_h=s64",
      "userId": "17533833776178224794"
     },
     "user_tz": -60
    },
    "id": "6-gXEDSX65bx",
    "outputId": "5491223b-5b50-48c2-8232-d75970e2c2ae"
   },
   "outputs": [],
   "source": [
    "print(index.getCollectionStatistics().toString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i6HrR4lc7i10"
   },
   "source": [
    "Let's unpack the statistics a bit more. We have 3 documents but why do we have only 4 unique terms? We can look at which terms we have by getting the [`Lexicon`](http://terrier.org/docs/current/javadoc/org/terrier/structures/Lexicon.html) object, which contains our vocabulary. We can iterate over the `Lexicon` from Python like a dictionary to see which terms are present and what information there is about each term after indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 585,
     "status": "ok",
     "timestamp": 1615971875714,
     "user": {
      "displayName": "Nicola Tonellotto",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gga8BaxLsPFWvzzBzSximki7T2Jsnf0EEARTd_h=s64",
      "userId": "17533833776178224794"
     },
     "user_tz": -60
    },
    "id": "us2mAzTW7Bny",
    "outputId": "e10d37e2-84a4-448d-9ed9-3d72a5ad5ab7"
   },
   "outputs": [],
   "source": [
    "for kv in index.getLexicon():\n",
    "    # Let's all print the type information of each to get a sense of what we're working with\n",
    "    print(\"%s (%s) -> %s (%s)\" % (kv.getKey(), type(kv.getKey()), kv.getValue().toString(), type(kv.getValue()) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fwbp94gh86pw"
   },
   "source": [
    "Iterating over the `Lexicon` shows that we're mapping a `String ` term to a [`LexiconEntry`](http://terrier.org/docs/current/javadoc/org/terrier/structures/LexiconEntry.html) object, which itself is an [`EntryStatistics`](http://terrier.org/docs/current/javadoc/org/terrier/structures/EntryStatistics.html). The `LexiconEntry` contains information including the statistics of that term.\n",
    "\n",
    "Looking at what we indexed reveals that PyTerrier is removing stopwords for us, much like Pyserini did. PyTerrier is also doing some token normalization as well so that we only have \"document\" in our index, even though document `d1` has the token \"documents\"! By default, Terrier removes standard stopwords and applies Porter's stemmer (which we talked about in class), though these behaviors can be configured.\n",
    "\n",
    "The `EntryStatistics` also provides a few other fields that offer insights:\n",
    " - `Nt` is the number of unique documents that each term occurs in – this is useful for calculating IDF.\n",
    " - `TF` is the total number of occurrences – some weighting models use this instead of Nt.\n",
    " - The numbers in the `@{}` are a pointer – they tell Terrier where the postings are for that term in the inverted index data structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTerrier also supports directly looking up a word using the `[]` operator, much like we would if we were looking up a key's value in a dictionary. Let's look up the value for the word \"document\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 37
    },
    "executionInfo": {
     "elapsed": 591,
     "status": "ok",
     "timestamp": 1615972070133,
     "user": {
      "displayName": "Nicola Tonellotto",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gga8BaxLsPFWvzzBzSximki7T2Jsnf0EEARTd_h=s64",
      "userId": "17533833776178224794"
     },
     "user_tz": -60
    },
    "id": "SZmi9498-Ijw",
    "outputId": "1dccc860-2f99-4e62-b1cc-da4918003b11"
   },
   "outputs": [],
   "source": [
    "print(index.getLexicon()[\"document\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vaKaU59l-kzg"
   },
   "source": [
    "We can use the information in the `Lexicon` to also look up documents as well. Remember from class that an inverted index is a mapping from a term to which *documents* each term occurs in. The `LexiconEntry` for a word contains the pointer to where to find the documents for that word in the inverted index. \n",
    "\n",
    "The object retrieved from using the `[]` operator with a `Lexicon` is a pointer that we can use with the inverted index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 640,
     "status": "ok",
     "timestamp": 1615972108524,
     "user": {
      "displayName": "Nicola Tonellotto",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gga8BaxLsPFWvzzBzSximki7T2Jsnf0EEARTd_h=s64",
      "userId": "17533833776178224794"
     },
     "user_tz": -60
    },
    "id": "XQki_Pds8ut2",
    "outputId": "f7824c65-33d9-499f-f9dc-97d8d7e652c5"
   },
   "outputs": [],
   "source": [
    "pointer = index.getLexicon()[\"document\"]\n",
    "for posting in index.getInvertedIndex().getPostings(pointer):\n",
    "    print(str(posting) + \" doclen=%d\" % posting.getDocumentLength())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7EaoIIO_DPx"
   },
   "source": [
    "From this output, we can see that the term \"document\" occurs in all three documents, as well as how long those documents are. Note that PyTerrier starts counting indexed documents with `int` values starting from 0 (called *docids*). These *docids* are then mapped back to *docnos*, which are the unique string identifiers for a document, e.g., the \"`d1`\", \"`d2`\" we used. This mapping is stored in a separate data structure called the *metaindex*, though you likely won't need to use that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOSdVAr-CGRf"
   },
   "source": [
    "## Searching an Index\n",
    "\n",
    "Our way into search in PyTerrier is called `BatchRetrieve`. BatchRetrieve is configured by specifying an index and a weighting model. Here', we'll use the `Tf` weighting, which is just term frequency; there are multiple possible weighting schemes, as we'll see later. Using a `BatchRetrieve` object, we will search for a single-word query, `\"document\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "executionInfo": {
     "elapsed": 1097,
     "status": "ok",
     "timestamp": 1615972796605,
     "user": {
      "displayName": "Nicola Tonellotto",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gga8BaxLsPFWvzzBzSximki7T2Jsnf0EEARTd_h=s64",
      "userId": "17533833776178224794"
     },
     "user_tz": -60
    },
    "id": "XtK93nwXCF5C",
    "outputId": "f10dccb8-7d91-44ba-8afa-b21b32022999"
   },
   "outputs": [],
   "source": [
    "br = pt.BatchRetrieve(index, wmodel=\"Tf\")\n",
    "br.search(\"document\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BHqSfTCtDM2T"
   },
   "source": [
    "The `search()` method returns a Pandas dataframe with columns:\n",
    " - `qid`: this is the query id, which is by default \"1\", since we issued only one query\n",
    " - `docid`: Terrier' internal integer for each document\n",
    " - `docno`: the external (string) unique identifier for each document\n",
    " - `score`: since we use the `Tf` weighting model, this score corresponds to the total frequency of the query (terms) in each document\n",
    " - `rank`: A handy attribute showing the descending order by score\n",
    " - `query`: the input query\n",
    "\n",
    "As expected, the `Tf` weighting model used here only counts the frequencies of the query terms in each document, i.e.:\n",
    "$$\n",
    "score(d,q) = \\sum_{t \\in q} tf_{t,d}\n",
    "$$\n",
    "\n",
    "Hence, it's clear that document `d1` should be the highest scored document with two occurrences (c.f. `'document'` and `'documents'`).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BJBXquPOD6q7"
   },
   "source": [
    "### Searching with multiple queries\n",
    "\n",
    "We can search for more than one query at a time using the  `transform()` method rather than the `search()` method. PyTerrier uses the notion of transformers, which we'll describe much more in Part 2, but for now, you can think of this function as transforming some input to some output. In our case, we'll create a Pandas DataFrame with our queries, which we'll provide as input to the `BatchRetrieve` object, to \"transform\" into results.\n",
    "\n",
    "Note that we not only need to provide queries, but also query identifiers in the `qid` column. These `qid` values will let us distinguish which results go to which query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = pd.DataFrame([[\"q1\", \"document\"], [\"q2\", \"first document\"]], columns=[\"qid\", \"query\"])\n",
    "queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can pass this queries data frame into `transform()` to get the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "executionInfo": {
     "elapsed": 566,
     "status": "ok",
     "timestamp": 1615972799893,
     "user": {
      "displayName": "Nicola Tonellotto",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gga8BaxLsPFWvzzBzSximki7T2Jsnf0EEARTd_h=s64",
      "userId": "17533833776178224794"
     },
     "user_tz": -60
    },
    "id": "TPBmPOETBKWk",
    "outputId": "69bf68d3-5e95-403d-f5f8-76bbd7cdaba9"
   },
   "outputs": [],
   "source": [
    "br.transform(queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tcgDzFLBEWAI"
   },
   "source": [
    "Most common operations in PyTerrier have to be overloaded so that you can call them using python syntax (called _operator overloading_). We'll discuss this more in Part 2, but for now, know that you can call `br.transform(queries)` using just `br(queries)`. Here. the `()` operator has been overloaded so that it calls `transform()` for us! You will see this usage very frequently in examples and documentation so it's worth noting and remembering the two are equivalent. As an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "executionInfo": {
     "elapsed": 655,
     "status": "ok",
     "timestamp": 1615972806683,
     "user": {
      "displayName": "Nicola Tonellotto",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gga8BaxLsPFWvzzBzSximki7T2Jsnf0EEARTd_h=s64",
      "userId": "17533833776178224794"
     },
     "user_tz": -60
    },
    "id": "YCwxb3HhEOp_",
    "outputId": "b0cfe94b-c1d7-4ac8-811d-befc9771ce32"
   },
   "outputs": [],
   "source": [
    "br(queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldY8VV8wQ60Z"
   },
   "source": [
    "## Scaling up to Covid-19 Data\n",
    "\n",
    "Let's move on to our full dataset, CORD19, which is easily accessible online. We'll use PyTerrier's `get_dataset()` function to download this corpus automatically and then to index it.\n",
    "\n",
    "### Task 1: Indexing data (5 points)\n",
    "\n",
    "You first task will be to write three lines of code that create the index using an indexer or, if the index was already created, loads the created index from file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 120273,
     "status": "ok",
     "timestamp": 1615972928650,
     "user": {
      "displayName": "Nicola Tonellotto",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gga8BaxLsPFWvzzBzSximki7T2Jsnf0EEARTd_h=s64",
      "userId": "17533833776178224794"
     },
     "user_tz": -60
    },
    "id": "L2lJsK-vEcQx",
    "outputId": "bcdfe9c5-7950-4974-a97d-ab3764995115"
   },
   "outputs": [],
   "source": [
    "cord19 = pt.datasets.get_dataset('irds:cord19/trec-covid')\n",
    "pt_index_path = './terrier_trec_covid'\n",
    "\n",
    "if not os.path.exists(pt_index_path + \"/data.properties\"):\n",
    "\n",
    "    # create the index, using the IterDictIndexer indexer \n",
    "\n",
    "    # TODO\n",
    "\n",
    "    # we give the dataset get_corpus_iter() directly to the indexer\n",
    "    # while specifying the fields to index and the metadata to record\n",
    "    \n",
    "    # TODO\n",
    "    pass\n",
    "else:\n",
    "    # if you already have the index, create an IndexRef from the data in pt_index_path\n",
    "    # that we can use to load using the IndexFactory\n",
    "    \n",
    "    # TODO\n",
    "    pass\n",
    "    \n",
    "index = pt.IndexFactory.of(index_ref)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y7GK9uANRt8w"
   },
   "source": [
    "### Task 2: 3 points\n",
    "- Print out the statistics of the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bNAVqf9uRr2p"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tQD9Q8CqSirN"
   },
   "source": [
    "As a curated collection, CORD19 has a corresponding set of queries, referred to as _topics_, and the relevance assessments for each query (i.e., topic), referred to as _qrels_. We use these to evaluate as a *test collection*. PyTerrier allows us to easily access the topics (queries) and qrels from the dataset. Like much of the inputs and outputs, these are expressed as dataframes as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "executionInfo": {
     "elapsed": 961,
     "status": "ok",
     "timestamp": 1615972942774,
     "user": {
      "displayName": "Nicola Tonellotto",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gga8BaxLsPFWvzzBzSximki7T2Jsnf0EEARTd_h=s64",
      "userId": "17533833776178224794"
     },
     "user_tz": -60
    },
    "id": "8n7oY-YYS_-A",
    "outputId": "6ff6f437-725f-4b92-bfab-787e18386941"
   },
   "outputs": [],
   "source": [
    "cord19.get_topics(variant='title').head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "executionInfo": {
     "elapsed": 1704,
     "status": "ok",
     "timestamp": 1615972945620,
     "user": {
      "displayName": "Nicola Tonellotto",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gga8BaxLsPFWvzzBzSximki7T2Jsnf0EEARTd_h=s64",
      "userId": "17533833776178224794"
     },
     "user_tz": -60
    },
    "id": "-rYxqvhJTGNX",
    "outputId": "e4db9772-f10f-4133-bc25-aee1a6f5a0ba"
   },
   "outputs": [],
   "source": [
    "cord19.get_qrels().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Gop4-jVbIIu"
   },
   "source": [
    "### Weighting Models\n",
    "\n",
    "In the earlier example, we used the simple \"`Tf`\" as our ranking function for document retrieval in BatchRetrieve. However, we can use other models such as `\"TF_IDF\"` by simply changing the `wmodel=\"Tf\"` keyword argument in the constructor of `BatchRetrieve`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "executionInfo": {
     "elapsed": 945,
     "status": "ok",
     "timestamp": 1615973062514,
     "user": {
      "displayName": "Nicola Tonellotto",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gga8BaxLsPFWvzzBzSximki7T2Jsnf0EEARTd_h=s64",
      "userId": "17533833776178224794"
     },
     "user_tz": -60
    },
    "id": "Cg8AGzCibdPG",
    "outputId": "bc353f41-1c33-468c-a834-37a93f1d0ced"
   },
   "outputs": [],
   "source": [
    "tfidf = pt.BatchRetrieve(index, wmodel=\"TF_IDF\")\n",
    "tfidf.search(\"chemical reactions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m6aZGX9sbdmc"
   },
   "source": [
    "Note that, as expected, because we switched the ranking, the scores of documents ranked by `TF_IDF` are no longer integers. You can see the exact TF-IDF formula used by Terrier from [the Github repo](https://github.com/terrier-org/terrier-core/blob/5.x/modules/core/src/main/java/org/terrier/matching/models/TF_IDF.java#L79)--sometimes helpful to know since there are multiple ways of defining TF-IDF! Terrier supports many weighting models and the documentation contains [a list of supported models](http://terrier.org/docs/current/javadoc/org/terrier/matching/models/package-summary.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YQ0j9lFfx-gO"
   },
   "source": [
    "## Evaluating and Comparing IR Models\n",
    "\n",
    "How do we know which of the models we've made so far are good IR models? PyTerrier provides a robust and extensive framework to help us automate the evaluation of IR models once we've defined them.\n",
    "\n",
    "As a first pass, let's take a look at the relevance scores in the dataset. To do this, we'll merge (`join`) the `qrels` with the results of our ranker to produce a dataframe that has both the ranking model's predictions (`\"score\"`) and the actual relevance score (`\"label\"`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "executionInfo": {
     "elapsed": 794,
     "status": "ok",
     "timestamp": 1615973109097,
     "user": {
      "displayName": "Nicola Tonellotto",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gga8BaxLsPFWvzzBzSximki7T2Jsnf0EEARTd_h=s64",
      "userId": "17533833776178224794"
     },
     "user_tz": -60
    },
    "id": "iyShZYpwwNSx",
    "outputId": "91f1ae8b-8f3a-4547-ae37-d9d808c970ce"
   },
   "outputs": [],
   "source": [
    "qrels = cord19.get_qrels()\n",
    "\n",
    "def get_res_with_labels(ranker, df):\n",
    "    # get the results for the query or queries\n",
    "    results = ranker( df )\n",
    "    # left outer join with the qrels\n",
    "    with_labels = results.merge(qrels, on=[\"qid\", \"docno\"], how=\"left\").fillna(0)\n",
    "    return with_labels\n",
    "\n",
    "# lets get the Tf results for the first query\n",
    "get_res_with_labels(tfidf, cord19.get_topics(variant='title').head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running an Experiment\n",
    "\n",
    "We don't actually need to produce that dataframe to do our evaluation though! PyTerrier lets us run different results with an [Experiment](https://pyterrier.readthedocs.io/en/latest/experiments.html) object, which will compare models according to the evaluation metrics we specify. Here, let's run an experiment to evaluate our `tfidf` model that we created earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "executionInfo": {
     "elapsed": 3826,
     "status": "ok",
     "timestamp": 1615973119937,
     "user": {
      "displayName": "Nicola Tonellotto",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gga8BaxLsPFWvzzBzSximki7T2Jsnf0EEARTd_h=s64",
      "userId": "17533833776178224794"
     },
     "user_tz": -60
    },
    "id": "OFUmiFSobUDg",
    "outputId": "db01e114-e6e7-4ff4-9447-ecf5e3d8ea54"
   },
   "outputs": [],
   "source": [
    "pt.Experiment(\n",
    "    [tfidf],\n",
    "    cord19.get_topics(variant='title'),\n",
    "    cord19.get_qrels(),\n",
    "    eval_metrics=[\"map\", \"ndcg\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mt0iPhRw2J-S"
   },
   "source": [
    "## Task 3: Define new models and evaluate them in an Experiment (28 points)\n",
    "\n",
    "Now comes the fun part! Your task is to define **three** new [`BatchRetrieve`](https://pyterrier.readthedocs.io/en/latest/terrier-retrieval.html#batchretrieve) objects with different word ranking methods. You are welcome to set the hyperparameters but all models should be sufficiently different. You are definitely welcome (encouraged, even!) to compare _more_ than three models too.\n",
    "\n",
    "Once you have defined your three `BatchRetrieve` objects, conduct an `Experiment` using all of them _at once_ (not three separate `Experiment` runs!) to evaluate the results.  Your experiment should include the two metrics used above, as well as NDCG for the top-5 and top-10 results. You are welcome to include other metrics as well\n",
    "\n",
    "Print the results of the Experiment and then write 2-3 sentences (or more) about what you see in the performance. Is there a clear better model? Would you expect better performance with some hyperparameter tuning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mRjyEZ5_aTvM"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ECIR 2021 Tutorial Notebook - Part 1.ipynb",
   "provenance": [
    {
     "file_id": "1kWCNf3QlQ4bX5YCM9OJBaaLikoTFCd5A",
     "timestamp": 1615914442515
    },
    {
     "file_id": "17Pihqt_C8DFzqlomTUks-5stNzNFjrAn",
     "timestamp": 1611078807322
    },
    {
     "file_id": "121AtOADdFd2VVAX5hcJX0WNBNt2_QHDu",
     "timestamp": 1609952873856
    },
    {
     "file_id": "1o4RTKOutf_FlMyPdEPkRyutnbY26JXMf",
     "timestamp": 1571324862553
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
