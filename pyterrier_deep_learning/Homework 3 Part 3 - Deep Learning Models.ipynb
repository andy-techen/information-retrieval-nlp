{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9egyhuvU3_GI"
   },
   "source": [
    "# SI 650 / EECS 549: Homework 3 Part 3\n",
    "\n",
    "Homework 3 Part 3 will have you working with deep learning models in a variety of ways. You will likely need to run this on Great Lakes unless you have access to a GPU elsewhere (or be prepared to wait a long time). You should have completed Parts 1 and 2 before attempting this notebook to familiarize yourself with how PyTerrier works.\n",
    "\n",
    "In Part 3, you'll try the following tasks:\n",
    " - Use a large language model to re-rank content\n",
    " - Use a text-to-text model to perform query augmentation\n",
    " - Train a deep learning IR model and compare its performance.\n",
    " \n",
    "The first two of these tasks will rely on models that we've pretrained for you. However, we've also provided code for how to train these. In the third task, you'll do one simple training and evaluate its results.\n",
    "\n",
    "For the first two tasks, we've provided most of the code. *You are expected to submit results showing that you have successfully executed it*. You'll need to understand some of the code to complete task 3, which requires you to write new code.\n",
    "\n",
    "As with the past notebooks, you'll need to have `JAVA_HOME` set, which will need to be run on Great Lakes. You can potentially set it in the notebook with a Jupyter command like\n",
    "```\n",
    "!export JAVA_HOME=/fill/in/the/path/to/the/JDK/here\n",
    "```\n",
    "by first figuring out where the JDK is installed. (This is setting the `JAVA_HOME` environment variable in the unix way). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mV0C6jJvqhMR"
   },
   "source": [
    "### Install the PyTerrier extensions\n",
    "\n",
    "You'll need two extensions for [OpenNIR](https://opennir.net/) and [doc2query](https://github.com/terrierteam/pyterrier_doc2query). We've provided the package install commands in comments below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AkIR_PXdet7R"
   },
   "outputs": [],
   "source": [
    "#pip install --upgrade git+https://github.com/Georgetown-IR-Lab/OpenNIR\n",
    "#pip install --upgrade git+https://github.com/terrierteam/pyterrier_doc2query.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A-nQrpNP5pN7"
   },
   "source": [
    "## Getting started\n",
    "\n",
    "Start PyTerrier as we have in past notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6FegcyWr5lja"
   },
   "outputs": [],
   "source": [
    "import pyterrier as pt\n",
    "if not pt.started():\n",
    "    pt.init(tqdm='notebook')\n",
    "import onir_pt\n",
    "import pyterrier_doc2query\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hPVjr448rIPc"
   },
   "source": [
    "### [TREC-COVID19](https://ir.nist.gov/covidSubmit/) Dataset download\n",
    "\n",
    "The following cell downloads the [TREC-COVID19](https://ir.nist.gov/covidSubmit/) dataset that we will use periodically throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IJMHFRfArd7O"
   },
   "outputs": [],
   "source": [
    "dataset = pt.datasets.get_dataset('irds:cord19/trec-covid')\n",
    "topics = dataset.get_topics(variant='description')\n",
    "qrels = dataset.get_qrels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OF3HIPhtrqOH"
   },
   "source": [
    "# Task 1: Build the inverted index for the TREC-COVID19 collection. (2 points)\n",
    "\n",
    "Build the index for the TREC Covid-19 (CORD19) data like we have in past notebooks but without any fancy options (e.g., no positional indexing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cord19 = pt.datasets.get_dataset('irds:cord19/trec-covid')\n",
    "pt_index_path = './terrier_cord19'\n",
    "\n",
    "if not os.path.exists(pt_index_path + \"/data.properties\"):\n",
    "    # TODO\n",
    "    \n",
    "    # create the index, using the IterDictIndexer indexer \n",
    "\n",
    "    # we give the dataset get_corpus_iter() directly to the indexer\n",
    "    # while specifying the fields to index and the metadata to record\n",
    "    pass\n",
    "\n",
    "else:\n",
    "    # TODO\n",
    "    \n",
    "    # if you already have the index, use it.\n",
    "    pass\n",
    "\n",
    "index = pt.IndexFactory.of(index_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rwDams5M7g6c"
   },
   "source": [
    "## Using an untuned Re-rankers\n",
    "\n",
    "This notebook will have you work with a few neural re-ranking methods that you've used in class. We can build them from scratch using `onir_pt.reranker` or load them from pretrained models. The models we load from scratch won't have been trained to do IR (yet), however.\n",
    "\n",
    "And OpenNIR reranking model consists of:\n",
    " - `ranker` (e.g., `drmm`, `knrm`, or `pacrr`). This defines the neural ranking architecture. We discussed the `knrm` approach in class.\n",
    " - `vocab` (e.g., `wordvec_hash`, or `bert`). This defines how text is encoded by the model. This approach makes it easy to swap out different text representations. \n",
    " \n",
    "Let's start with the `knrm` method we discussed in class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F0O79K2K6fvn"
   },
   "outputs": [],
   "source": [
    "knrm = onir_pt.reranker('knrm', 'wordvec_hash', text_field='abstract')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x1avVTpxDORN"
   },
   "source": [
    "Let's look at how well this model work at ranking compared with our default `BatchRetrieve`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4FWUIN577v1O"
   },
   "outputs": [],
   "source": [
    "br = pt.BatchRetrieve(index) % 100\n",
    "pipeline = br >> pt.text.get_text(dataset, 'abstract') >> knrm\n",
    "pt.Experiment(\n",
    "    [br, pipeline],\n",
    "    topics,\n",
    "    qrels,\n",
    "    names=['DPH', 'DPH >> KNRM'],\n",
    "    eval_metrics=[\"map\", \"ndcg\", 'ndcg_cut.10', 'P.10', 'mrt']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MhquobQypVNJ"
   },
   "source": [
    "The `knrm` models' performance is lower! The mode doesn't work very well because it hasn't yet been trained for IR; it's using random weights to combine the scores from the similarity matrix--but this is at least a start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WLv7quA43yAP"
   },
   "source": [
    "## Loading a trained re-ranker\n",
    "\n",
    "You can train re-ranking models in PyTerrier using the `fit` method. Here's an example of how to train the `knrm` model on the MS MARCO dataset, which is a large IR collection.\n",
    "\n",
    "```python\n",
    "# transfer training signals from a medical sample of MS MARCO\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_ds = pt.datasets.get_dataset('irds:msmarco-passage/train/medical')\n",
    "train_topics, valid_topics = train_test_split(train_ds.get_topics(), test_size=50, random_state=42) # split into training and validation sets\n",
    "\n",
    "# Index MS MARCO\n",
    "indexer = pt.index.IterDictIndexer('./terrier_msmarco-passage')\n",
    "tr_index_ref = indexer.index(train_ds.get_corpus_iter(), fields=('text',), meta=('docno',))\n",
    "\n",
    "pipeline = (pt.BatchRetrieve(tr_index_ref) % 100 # get top 100 results\n",
    "            >> pt.text.get_text(train_ds, 'text') # fetch the document text\n",
    "            >> pt.apply.generic(lambda df: df.rename(columns={'text': 'abstract'})) # rename columns\n",
    "            >> knrm) # apply neural re-ranker\n",
    "\n",
    "pipeline.fit(\n",
    "    train_topics,\n",
    "    train_ds.get_qrels(),\n",
    "    valid_topics,\n",
    "    train_ds.get_qrels())\n",
    "```\n",
    "\n",
    "Training deep learning models takes a bit of time (especially for large datasets like MS MARCO), so we've provided a model that's already been trained for you to download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yk7FBOgvDa8V"
   },
   "outputs": [],
   "source": [
    "del knrm # free up the memory before loading a new version of the ranker (helpful for the GPU)\n",
    "knrm = onir_pt.reranker.from_checkpoint('http://jurgens.people.si.umich.edu/ir/knrm.medmarco.tar.gz', text_field='abstract', \n",
    "                                        expected_md5=\"d70b1d4f899690dae51161537e69ed5a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1BQQKv8lL0Ta"
   },
   "outputs": [],
   "source": [
    "pipeline2 = br >> pt.text.get_text(dataset, 'abstract') >> knrm\n",
    "pt.Experiment(\n",
    "    [br, pipeline2],\n",
    "    topics,\n",
    "    qrels,\n",
    "    names=['DPH', 'DPH >> KNRM'],\n",
    "    baseline=0,\n",
    "    eval_metrics=[\"map\", \"ndcg\", 'ndcg_cut.10', 'P.10', 'mrt']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aI1_8O8rtXKK"
   },
   "source": [
    "The tuned performance is a little better than before, but `knrm` still underperforms our first-stage ranking model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79l1jn0pRQEY"
   },
   "source": [
    "## Reranking with BERT\n",
    "\n",
    "Large language models such as [BERT](https://arxiv.org/abs/1810.04805) are much more powerful neural models that have been shown to be effective for ranking like we discussed in class. \n",
    "\n",
    "Like with `knrm`, we'll start by using BERT for re-ranking with its initial parameters. These parameters have been turned for the masked language modeling (i.e., filling a word in the blank) and predicting the next sentence--but have not been tuned for IR at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-qlXPHqN3iO0"
   },
   "outputs": [],
   "source": [
    "del knrm # clear out memory from KNRM (useful for GPU)\n",
    "bert = onir_pt.reranker('vanilla_transformer', 'bert', text_field='abstract', vocab_config={'train': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "progrVwaunrn"
   },
   "source": [
    "Let's see how this non-IR trained model does on CORD10 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PkasovrjQjy0"
   },
   "outputs": [],
   "source": [
    "pipeline3 = br % 100 >> pt.text.get_text(dataset, 'abstract') >> bert\n",
    "pt.Experiment(\n",
    "    [br, pipeline3],\n",
    "    topics,\n",
    "    qrels,\n",
    "    names=['DPH', 'DPH >> VBERT'],\n",
    "    baseline=0,\n",
    "    eval_metrics=[\"map\", \"ndcg\", 'ndcg_cut.10', 'P.10', 'mrt']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wBrfNZ_1u_pD"
   },
   "source": [
    "As we see, although the ERT model is pre-trained for recognizing language, it doesn't do very well at ranking on our benchmark. To get better performance, we'll need to tune for the task of relevance ranking.\n",
    "\n",
    "We can train the model for ranking (as shown above for KNRM) or we can download a trained model. Here, we will use the [SLEDGE](https://arxiv.org/abs/2010.05987) model, which is a BERT model trained on scientific text and tuned on medical queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VsXQKNyYSXOj"
   },
   "outputs": [],
   "source": [
    "bert = onir_pt.reranker.from_checkpoint('http://jurgens.people.si.umich.edu/ir/scibert-medmarco.tar.gz', \n",
    "                                         text_field='abstract', expected_md5=\"854966d0b61543ffffa44cea627ab63b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run another experiment to see how this new model trained for IR does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dUH-daNJSoNy"
   },
   "outputs": [],
   "source": [
    "pipeline4 = br % 100 >> pt.text.get_text(dataset, 'abstract') >> bert\n",
    "pt.Experiment(\n",
    "    [br, pipeline4],\n",
    "    topics,\n",
    "    qrels,\n",
    "    names=['DPH', 'DPH >> Trained-BERT'],\n",
    "    baseline=0,\n",
    "    eval_metrics=[\"map\", \"ndcg\", 'ndcg_cut.10', 'P.10', 'mrt']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OtAxDQHyv4ON"
   },
   "source": [
    "Training helped a lot! We're able to improve upon the initial ranking from `BatchRetrieve`. However, from looking at `mrt` we can see that this is pretty slow to run--and this was using a GPU! This performance time underscores the trade-off in using large language models at retrieval time: they may perform better, but could be much slower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning at indexing time: doc2query\n",
    "\n",
    "Instead of using our large language models to rerank, another option is to use them at _indexing time_ to augment our documents. In class, we discussed one such option, doc2query, that augments an inverted index structure by predicting queries that may be used to search for the document, and appending those to the document text.\n",
    "\n",
    "We can use doc2query using the `pyterrier_doc2query` package, which was loaded at the top."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a pre-trained model\n",
    "\n",
    "We'll start by using a version of the doc2query model released by the authors that is trained on the MS MARCO collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"t5-base.zip\"):\n",
    "  !wget http://jurgens.people.si.umich.edu/ir/t5-base.zip\n",
    "  !unzip t5-base.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load the model weights by specifying the checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2query = pyterrier_doc2query.Doc2Query('model.ckpt-1004000', batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running doc2queries on sample text\n",
    "\n",
    "Let's see what queries it predicts for the sample document that we've made up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([{\"docno\" : \"d1\", \"text\" :\"The University of Michigan School of Information (UMSI) delivers innovative, elegant and ethical solutions connecting people, information and technology. The school was one of the first iSchools in the nation and is the premier institution studying and using technology to improve human computer interactions.\"}])\n",
    "df.iloc[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2query_df = doc2query(df)\n",
    "doc2query_df.iloc[0].querygen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too bad, though the questions are somewhat generic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading an index of doc2query documents\n",
    "\n",
    "Let's see how doc2query does on improving the performance in the TREC COVID data. Since indexing with doc2query takes a while (due to needing to run the deep learning models), we've provided an index with the text already added.\n",
    "\n",
    "If you would like to index the collection with doc2query yourself (or use doc2query for your course project), you can use the following code:\n",
    "\n",
    "```python\n",
    "dataset = pt.get_dataset(\"irds:cord19/trec-covid\")\n",
    "indexer = (\n",
    "  pyterrier_doc2query.Doc2Query('model.ckpt-1004000', doc_attr='abstract', batch_size=8, append=True) # aply doc2query on abstracts and append\n",
    "  >> pt.apply.generic(lambda df: df.rename(columns={'abstract': 'text'}) # rename \"abstract\" column to \"text\" for indexing\n",
    "  >> pt.IterDictIndexer(\"./doc2query_index_path\")) # index the expanded documents\n",
    "indexref = indexer.index(dataset.get_corpus_iter())\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('doc2query_marco_cord19.zip'):\n",
    "  !wget http://jurgens.people.si.umich.edu/ir/doc2query_marco_cord19.zip\n",
    "  !unzip doc2query_marco_cord19.zip\n",
    "doc2query_indexref = pt.IndexRef.of('./doc2query_index_path/data.properties')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the results on TREC COVID by first merging the scores with the rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pt.get_dataset('irds:cord19/trec-covid')\n",
    "pipeline = pt.BatchRetrieve(doc2query_indexref) % 1 >> pt.text.get_text(dataset, 'title')\n",
    "res = pipeline(dataset.get_topics('title'))\n",
    "res.merge(dataset.get_qrels(), how='left').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What kind of queries does doc2query generate for the CORD19 documents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(doc for doc in dataset.get_corpus_iter() if doc['docno'] in ('3sepefqa', 'l5fxswfz'))\n",
    "df = df.rename(columns={'abstract': 'text'})\n",
    "doc2query_df = doc2query(df)\n",
    "for querygen, docno, text in zip(doc2query_df['querygen'], doc2query_df['docno'], df['text']):\n",
    "    print(docno)\n",
    "    print(querygen)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the effects of doc2query\n",
    "\n",
    "Here, we'll change our evaluation setup a bit from what we did before. Rather than compare two models for the same index, we'll instead compare the same model (BM25) with two different ways of indexing (two indices)! Our baseline will be an index of CORD19 without the doc2query additions.\n",
    "\n",
    "Let's load a copy of the CORD19 index that we used earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexref = pt.IndexRef.of('./terrier_cord19/data.properties')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Write the `Experiment` to compare indices (3 points)\n",
    "Run an `Experiment` using a `BM25` ranker that compares the indices `indexref` and `doc2query_indexref`. Compare your models using MAP, NDCG, and NDCG@10. Note that our doc2query model was trained on MS MARCO, which isn't the same kind of collection as CORD19, so this performance tells us how well that model can transfer to a new setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Train a new model! (25 points)\n",
    "\n",
    "All of the prior exercises have had you working with either off-the-shelf models (not trained for IR) or models that someone else has trained for you. To give you a sense of how to train a model, your primary task in this notebook is to train a simple `knrm` model, which should be relatively efficient to train on a GPU. \n",
    "\n",
    "To keep things simple, we'll use the same setup for CORD19 that we did in Part 2 (30 queries in train, 5 in dev, 15 queries in test) which is still relatively small for training a deep learning model but will get you started on the process. \n",
    "\n",
    "Your tasks are the following:\n",
    "- Load the CORD19 dataset and split it into train, dev, and test\n",
    "- Create a new `knrm` ranker and a pipeline that uses it\n",
    "- Run an `Experiment` comparing four models:\n",
    "  - a default `BatchRetrieve`, filtering to the top 100 results\n",
    "  - BM25, filtering to the top 100 results\n",
    "  - a pipeline that feeds the top 100 results of the default `BatchRetrieve` to your `knrm` model\n",
    "  - a pipeline that feeds the top 100 results of BM25 to your `knrm` model\n",
    "  \n",
    "Your `Experiment` should evaluate models using MAP, NDCG, NDCG@10, Precision@10, and Mean Response Time.\n",
    "  \n",
    "We expect to see the `Experiment`'s results in the final cell. You are, of course, welcome to try training any of the fancier models to see how they do as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "neural ir.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
